{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def simclr_loss(z: torch.Tensor, temperature: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute SimCLR (NT-Xent) loss.\n",
    "\n",
    "    Args:\n",
    "        z (torch.Tensor): Tensor of shape (2N, D), where 2N is the number of augmented samples,\n",
    "                          and D is the embedding dimension.\n",
    "                          The first N and last N are positive pairs.\n",
    "        temperature (float): Temperature parameter Ï„.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The scalar loss value.\n",
    "    \"\"\"\n",
    "    print(z)\n",
    "    # Normalize the representations\n",
    "    z = F.normalize(z, dim=1)\n",
    "    print(z)\n",
    "    # Compute cosine similarity matrix (2N x 2N)\n",
    "    similarity_matrix = torch.matmul(z, z.T)\n",
    "    print(similarity_matrix)\n",
    "    # Get batch size\n",
    "    batch_size = z.size(0)\n",
    "    assert batch_size % 2 == 0, \"Batch size should be even for SimCLR\"\n",
    "    N = batch_size // 2\n",
    "\n",
    "    # Create labels for positive pairs: (0, N), (1, N+1), ..., (N-1, 2N-1)\n",
    "    pos_indices = torch.arange(N)\n",
    "    print(pos_indices)\n",
    "    positives = torch.cat([\n",
    "        torch.stack([pos_indices, pos_indices + N], dim=1),\n",
    "        torch.stack([pos_indices + N, pos_indices], dim=1)\n",
    "    ], dim=0)\n",
    "    print(positives)\n",
    "    # Mask to remove self-comparisons from denominator\n",
    "    mask = torch.eye(batch_size, dtype=torch.bool, device=z.device)\n",
    "    print(mask)\n",
    "    logits = similarity_matrix / temperature\n",
    "    print(logits)\n",
    "    logits.masked_fill_(mask, -1e9)\n",
    "\n",
    "\n",
    "    # Compute log-softmax\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    softmax = F.softmax(logits, dim=1)\n",
    "    print(\"log probability\",log_probs)\n",
    "    print(\"softmax\", softmax)\n",
    "    # Compute the mean loss over all positive pairs\n",
    "    loss = -log_probs[positives[:, 0], positives[:, 1]]\n",
    "    print(loss)\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9029e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(z1, z2, temperature=.5):\n",
    "    batch_size = z1.shape[0]\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    \n",
    "    # Concatenate\n",
    "    z = torch.cat([z1, z2], dim=0)  # (2N, D)\n",
    "    \n",
    "    # Similarity matrix\n",
    "    sim_matrix = torch.matmul(z, z.T)  # (2N, 2N)\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "\n",
    "    # Remove similarity of samples to themselves\n",
    "    mask = torch.eye(sim_matrix.size(0), device=z.device).bool()\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, -float('inf'))\n",
    "\n",
    "    # Positive pairs (i, i+N) and (i+N, i)\n",
    "    positives = torch.cat([\n",
    "        torch.arange(batch_size, device=z.device) + batch_size,\n",
    "        torch.arange(batch_size, device=z.device)\n",
    "    ])\n",
    "\n",
    "    # Labels\n",
    "    labels = positives\n",
    "\n",
    "    # Loss\n",
    "    loss = F.cross_entropy(sim_matrix, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f877ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = torch.randn(4,2)\n",
    "batch_1 = original @ (torch.ones(2,2)*0.5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0bfc752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5662,  0.3491],\n",
       "         [-1.2195, -1.2805],\n",
       "         [-0.3878,  1.1199],\n",
       "         [ 1.8077,  1.1549]]),\n",
       " tensor([[-0.1086, -0.1086],\n",
       "         [-1.2500, -1.2500],\n",
       "         [ 0.3660,  0.3660],\n",
       "         [ 1.4813,  1.4813]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original, batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c698568",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_2 = original+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6760d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4338, 2.3491],\n",
       "        [0.7805, 0.7195],\n",
       "        [1.6122, 3.1199],\n",
       "        [3.8077, 3.1549]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23bd9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1086, -0.1086],\n",
      "        [-1.2500, -1.2500],\n",
      "        [ 0.3660,  0.3660],\n",
      "        [ 1.4813,  1.4813],\n",
      "        [ 1.4338,  2.3491],\n",
      "        [ 0.7805,  0.7195],\n",
      "        [ 1.6122,  3.1199],\n",
      "        [ 3.8077,  3.1549]])\n",
      "tensor([[-0.7071, -0.7071],\n",
      "        [-0.7071, -0.7071],\n",
      "        [ 0.7071,  0.7071],\n",
      "        [ 0.7071,  0.7071],\n",
      "        [ 0.5210,  0.8536],\n",
      "        [ 0.7353,  0.6778],\n",
      "        [ 0.4591,  0.8884],\n",
      "        [ 0.7700,  0.6380]])\n",
      "tensor([[ 1.0000,  1.0000, -1.0000, -1.0000, -0.9720, -0.9992, -0.9528, -0.9956],\n",
      "        [ 1.0000,  1.0000, -1.0000, -1.0000, -0.9720, -0.9992, -0.9528, -0.9956],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000,  0.9720,  0.9992,  0.9528,  0.9956],\n",
      "        [-1.0000, -1.0000,  1.0000,  1.0000,  0.9720,  0.9992,  0.9528,  0.9956],\n",
      "        [-0.9720, -0.9720,  0.9720,  0.9720,  1.0000,  0.9616,  0.9975,  0.9458],\n",
      "        [-0.9992, -0.9992,  0.9992,  0.9992,  0.9616,  1.0000,  0.9397,  0.9986],\n",
      "        [-0.9528, -0.9528,  0.9528,  0.9528,  0.9975,  0.9397,  1.0000,  0.9203],\n",
      "        [-0.9956, -0.9956,  0.9956,  0.9956,  0.9458,  0.9986,  0.9203,  1.0000]])\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([[0, 4],\n",
      "        [1, 5],\n",
      "        [2, 6],\n",
      "        [3, 7],\n",
      "        [4, 0],\n",
      "        [5, 1],\n",
      "        [6, 2],\n",
      "        [7, 3]])\n",
      "tensor([[ True, False, False, False, False, False, False, False],\n",
      "        [False,  True, False, False, False, False, False, False],\n",
      "        [False, False,  True, False, False, False, False, False],\n",
      "        [False, False, False,  True, False, False, False, False],\n",
      "        [False, False, False, False,  True, False, False, False],\n",
      "        [False, False, False, False, False,  True, False, False],\n",
      "        [False, False, False, False, False, False,  True, False],\n",
      "        [False, False, False, False, False, False, False,  True]])\n",
      "tensor([[ 2.0000,  2.0000, -2.0000, -2.0000, -1.9439, -1.9983, -1.9056, -1.9913],\n",
      "        [ 2.0000,  2.0000, -2.0000, -2.0000, -1.9439, -1.9983, -1.9056, -1.9913],\n",
      "        [-2.0000, -2.0000,  2.0000,  2.0000,  1.9439,  1.9983,  1.9056,  1.9913],\n",
      "        [-2.0000, -2.0000,  2.0000,  2.0000,  1.9439,  1.9983,  1.9056,  1.9913],\n",
      "        [-1.9439, -1.9439,  1.9439,  1.9439,  2.0000,  1.9232,  1.9950,  1.8915],\n",
      "        [-1.9983, -1.9983,  1.9983,  1.9983,  1.9232,  2.0000,  1.8794,  1.9972],\n",
      "        [-1.9056, -1.9056,  1.9056,  1.9056,  1.9950,  1.8794,  2.0000,  1.8406],\n",
      "        [-1.9913, -1.9913,  1.9913,  1.9913,  1.8915,  1.9972,  1.8406,  2.0000]])\n",
      "log probability tensor([[-1.0000e+09, -1.0702e-01, -4.1070e+00, -4.1070e+00, -4.0509e+00,\n",
      "         -4.1054e+00, -4.0126e+00, -4.0983e+00],\n",
      "        [-1.0702e-01, -1.0000e+09, -4.1070e+00, -4.1070e+00, -4.0509e+00,\n",
      "         -4.1054e+00, -4.0126e+00, -4.0983e+00],\n",
      "        [-5.5855e+00, -5.5855e+00, -1.0000e+09, -1.5855e+00, -1.6416e+00,\n",
      "         -1.5871e+00, -1.6799e+00, -1.5942e+00],\n",
      "        [-5.5855e+00, -5.5855e+00, -1.5855e+00, -1.0000e+09, -1.6416e+00,\n",
      "         -1.5871e+00, -1.6799e+00, -1.5942e+00],\n",
      "        [-5.5016e+00, -5.5016e+00, -1.6138e+00, -1.6138e+00, -1.0000e+09,\n",
      "         -1.6345e+00, -1.5627e+00, -1.6662e+00],\n",
      "        [-5.5759e+00, -5.5759e+00, -1.5792e+00, -1.5792e+00, -1.6544e+00,\n",
      "         -1.0000e+09, -1.6982e+00, -1.5803e+00],\n",
      "        [-5.4304e+00, -5.4304e+00, -1.6192e+00, -1.6192e+00, -1.5298e+00,\n",
      "         -1.6454e+00, -1.0000e+09, -1.6842e+00],\n",
      "        [-5.5529e+00, -5.5529e+00, -1.5704e+00, -1.5704e+00, -1.6701e+00,\n",
      "         -1.5644e+00, -1.7210e+00, -1.0000e+09]])\n",
      "softmax tensor([[0.0000, 0.8985, 0.0165, 0.0165, 0.0174, 0.0165, 0.0181, 0.0166],\n",
      "        [0.8985, 0.0000, 0.0165, 0.0165, 0.0174, 0.0165, 0.0181, 0.0166],\n",
      "        [0.0038, 0.0038, 0.0000, 0.2048, 0.1937, 0.2045, 0.1864, 0.2031],\n",
      "        [0.0038, 0.0038, 0.2048, 0.0000, 0.1937, 0.2045, 0.1864, 0.2031],\n",
      "        [0.0041, 0.0041, 0.1991, 0.1991, 0.0000, 0.1950, 0.2096, 0.1890],\n",
      "        [0.0038, 0.0038, 0.2061, 0.2061, 0.1912, 0.0000, 0.1830, 0.2059],\n",
      "        [0.0044, 0.0044, 0.1981, 0.1981, 0.2166, 0.1929, 0.0000, 0.1856],\n",
      "        [0.0039, 0.0039, 0.2080, 0.2080, 0.1882, 0.2092, 0.1789, 0.0000]])\n",
      "tensor([4.0509, 4.1054, 1.6799, 1.5942, 5.5016, 5.5759, 1.6192, 1.5704])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.2122)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simclr_loss(torch.cat([batch_1, batch_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d004d3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2122)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_xent_loss(batch_1, batch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "548a7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def ntxent_loss_with_labels(out0, labels, temperature=0.5):\n",
    "    \"\"\"\n",
    "    NT-Xent loss for a batch with labels indicating class/group similarity.\n",
    "\n",
    "    Args:\n",
    "        out0 (Tensor): Output embeddings for a batch of images (batch_size, embedding_size).\n",
    "        labels (Tensor): Labels for the batch (batch_size,). Each label indicates the class/group of each sample.\n",
    "        temperature (float): Scaling factor for logits.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Contrastive Cross-Entropy Loss value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize the output to unit length (cosine similarity)\n",
    "    out0 = nn.functional.normalize(out0, dim=1)\n",
    "\n",
    "    # Calculate cosine similarity (pairwise similarity matrix)\n",
    "    logits = torch.einsum(\"nc,mc->nm\", out0, out0) / temperature\n",
    "    print(logits)\n",
    "    # Mask diagonal (self-similarity)\n",
    "    batch_size = out0.size(0)\n",
    "    mask = torch.eye(batch_size, device=out0.device, dtype=torch.bool)\n",
    "    logits = logits[~mask].view(batch_size, -1)  # Remove self-similarities\n",
    "\n",
    "    # Generate positive pair labels: Same label is a positive pair\n",
    "    labels = labels.unsqueeze(0) == labels.unsqueeze(1)  # Shape: (batch_size, batch_size)\n",
    "    print(labels)\n",
    "    # Convert boolean mask to integer (1 for positive pairs, 0 for negative pairs)\n",
    "    labels = labels.float()\n",
    "\n",
    "    # Cross-entropy loss: maximize similarity for positive pairs, minimize for negative pairs\n",
    "    cross_entropy = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    print(labels)\n",
    "    # The target labels should be indices where labels are 1 (positive pairs)\n",
    "    target = labels.argmax(dim=1)  # Get the index of the positive pair for each sample\n",
    "    print(target)\n",
    "    # Calculate the loss\n",
    "    loss = cross_entropy(logits, target)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5dc53d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0000,  1.2998,  1.8844,  0.1167,  1.8589,  1.3779,  0.9956,  1.9544],\n",
      "        [ 1.2998,  2.0000,  1.7340, -1.4416,  1.7690,  1.9972,  1.9653,  1.5928],\n",
      "        [ 1.8844,  1.7340,  2.0000, -0.5590,  1.9987,  1.7840,  1.5192,  1.9837],\n",
      "        [ 0.1167, -1.4416, -0.5590,  2.0000, -0.6282, -1.3667, -1.6735, -0.3097],\n",
      "        [ 1.8589,  1.7690,  1.9987, -0.6282,  2.0000,  1.8155,  1.5653,  1.9732],\n",
      "        [ 1.3779,  1.9972,  1.7840, -1.3667,  1.8155,  2.0000,  1.9431,  1.6542],\n",
      "        [ 0.9956,  1.9653,  1.5192, -1.6735,  1.5653,  1.9431,  2.0000,  1.3411],\n",
      "        [ 1.9544,  1.5928,  1.9837, -0.3097,  1.9732,  1.6542,  1.3411,  2.0000]])\n",
      "tensor([[ True, False, False, False, False,  True, False, False],\n",
      "        [False,  True, False, False,  True, False, False, False],\n",
      "        [False, False,  True,  True, False, False, False, False],\n",
      "        [False, False,  True,  True, False, False, False, False],\n",
      "        [False,  True, False, False,  True, False, False, False],\n",
      "        [ True, False, False, False, False,  True, False, False],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True]])\n",
      "tensor([[1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1.]])\n",
      "tensor([0, 1, 2, 2, 1, 0, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3013)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_3 = torch.randn(8,2)\n",
    "labels = torch.tensor([1,2,3,3,2,1,4,4])\n",
    "\n",
    "ntxent_loss_with_labels(batch_3, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e371e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
